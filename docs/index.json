[{"authors":["amadu"],"categories":null,"content":"Ahmadu Shehu is a trained Linguist who holds a First Class degree, Masters and Ph.D. in Linguistics from the Universities of Maiduguri, Malaya and Warsaw, respectively. He has nearly two decades of experience in teaching languages and linguistics at primary, secondary and university levels. Ahmadu is currently an Assistant Professor of English and Literature at the American University of Nigeria, Yola. He had previously taught Linguistics and Nigerian languages, especially Hausa and Fulfulde at Bayero University, Kano, and was a visiting scholar at the Universities of Hamburg, Cologne and Vienna where he conducted research and presented numerous lectures. His research works, publications and interests cover various theories of Cognitive Linguistics, Cultural Linguistics and structural aspects of African languages. He has also participated in European funded scholarships and research grants with excellent outputs.\n body { text-align: justify}  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"42a6b23ab7e8a444b80404b39d1bcdd6","permalink":"https://hausanlp.github.io/author/ahamdu-shehu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ahamdu-shehu/","section":"authors","summary":"Ahmadu Shehu is a trained Linguist who holds a First Class degree, Masters and Ph.D. in Linguistics from the Universities of Maiduguri, Malaya and Warsaw, respectively. He has nearly two decades of experience in teaching languages and linguistics at primary, secondary and university levels.","tags":null,"title":"Ahamdu Shehu","type":"authors"},{"authors":["bsbello"],"categories":null,"content":"Bello Shehu Bello is a lecturer at the Department of Computer Science Bayero University Kano. He obtained his PhD in Computer Science from University of Leicester, United Kingdom. His research interests focus on Social media analysis, Machine Learning and Computational Social Science. Bello has worked and published articles on Social media and Artificial Intelligence, particularly the use of automated accounts on social media for election campaigns, opinion manipulation and spread of propaganda.\n body { text-align: justify}  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1ed14a45a59844d34571b9ebb1f73696","permalink":"https://hausanlp.github.io/author/bello-shehu-bello/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bello-shehu-bello/","section":"authors","summary":"Bello Shehu Bello is a lecturer at the Department of Computer Science Bayero University Kano. He obtained his PhD in Computer Science from University of Leicester, United Kingdom. His research interests focus on Social media analysis, Machine Learning and Computational Social Science.","tags":null,"title":"Bello Shehu Bello","type":"authors"},{"authors":["Ibrahim"],"categories":null,"content":"Ibrahim Said Ahmad is lecturer in the Department of Information Technology, Bayero University Kano. He completed his PhD from Universiti Kebangsaan Malaysia, in 2020 focusing on data science. His main areas of interest lie in Data Analytics, Natural Language Processing and Artificial Intelligence specifically in business intelligence and computational intelligence. He has worked and published articles on sentiment analysis, natural language processing, and data mining.\n body { text-align: justify}  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0709bfc8ff758100f78976b6dc3622dd","permalink":"https://hausanlp.github.io/author/ibrahim-said-ahmad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ibrahim-said-ahmad/","section":"authors","summary":"Ibrahim Said Ahmad is lecturer in the Department of Information Technology, Bayero University Kano. He completed his PhD from Universiti Kebangsaan Malaysia, in 2020 focusing on data science. His main areas of interest lie in Data Analytics, Natural Language Processing and Artificial Intelligence specifically in business intelligence and computational intelligence.","tags":null,"title":"Ibrahim Said Ahmad","type":"authors"},{"authors":["IdrisAbdul"],"categories":null,"content":"Idris Abdulmumin is a Bachelor of Science graduate in Computer Science from Bayero University Kano, an MSc holder of Information Technology Security from Nottingham Trent University and a candidate for PhD in Computer Science with specialization in Neural Machine Translation for Low Resource Languages in the Department of Computer Science, Bayero University Kano.\n body { text-align: justify}  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6fdc83dfd5e607558afa031fa02aac8f","permalink":"https://hausanlp.github.io/author/idris-abdulmuminu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/idris-abdulmuminu/","section":"authors","summary":"Idris Abdulmumin is a Bachelor of Science graduate in Computer Science from Bayero University Kano, an MSc holder of Information Technology Security from Nottingham Trent University and a candidate for PhD in Computer Science with specialization in Neural Machine Translation for Low Resource Languages in the Department of Computer Science, Bayero University Kano.","tags":null,"title":"Idris Abdulmuminu","type":"authors"},{"authors":["jaafar"],"categories":null,"content":"Jaafar Zubairu Maitama is lecturer in the Department of Information Technology, Bayero University Kano. He received a B.Sc. degree in Computer Science from Bayero University, Kano, Nigeria in 2011 and a Master’s degree in Computer Science (Artificial Intelligence) in 2015 at University of Malaya, Malaysia. He is currently a PhD researcher in Artificial Intelligence Department, University of Malaya. His research interest includes natural language processing, summarization, machine learning, and sentiment analysis\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"96b3162c522e31148fd3f359c183f13c","permalink":"https://hausanlp.github.io/author/jaafar-zubairu-maitama/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jaafar-zubairu-maitama/","section":"authors","summary":"Jaafar Zubairu Maitama is lecturer in the Department of Information Technology, Bayero University Kano. He received a B.Sc. degree in Computer Science from Bayero University, Kano, Nigeria in 2011 and a Master’s degree in Computer Science (Artificial Intelligence) in 2015 at University of Malaya, Malaysia.","tags":null,"title":"Jaafar Zubairu Maitama","type":"authors"},{"authors":["shamsu"],"categories":null,"content":"Shamsuddeen Muhammad is a PhD candidate at the MAPi Doctoral Program in Computer Science, a joint Ph.D. program offered by the University of Minho, the University of Aveiro, and the University of Porto. He is also a researcher at the Laboratory of Artificial Intelligence and Decision Support LIAAD-INESTEC. His research interest focus on Machine Learning and Natural Language Processing. He received his master’s degree from the University of Manchester, UK and he is a faculty member at the faculty of computer science and information technology, Bayero University, Kano-Nigeria. He loves reading and learning a new concept every day.\n body { text-align: justify}  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6e67e6d9781c61dba49aa2c6f4913c90","permalink":"https://hausanlp.github.io/author/shamsuddeen-hassan-muhammad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shamsuddeen-hassan-muhammad/","section":"authors","summary":"Shamsuddeen Muhammad is a PhD candidate at the MAPi Doctoral Program in Computer Science, a joint Ph.D. program offered by the University of Minho, the University of Aveiro, and the University of Porto.","tags":null,"title":"Shamsuddeen Hassan Muhammad","type":"authors"},{"authors":["Suhail"],"categories":null,"content":"Suhail Kamal is a lecturer in the Department of Information Technology in the Faculty of Computer Science and Information Technology, BUK. He obtained his bachelor\u0026rsquo;s and master\u0026rsquo;s degree in 2011 and 2014, respectively. He is currently a PhD student in Computer Science and Technology in Xiamen University, China. His research interests include Sign Language Recognition and Translation.\n body { text-align: justify}  ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d90629ec4b2d735e79cf816af2d6506b","permalink":"https://hausanlp.github.io/author/suhail-kamal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/suhail-kamal/","section":"authors","summary":"Suhail Kamal is a lecturer in the Department of Information Technology in the Faculty of Computer Science and Information Technology, BUK. He obtained his bachelor\u0026rsquo;s and master\u0026rsquo;s degree in 2011 and 2014, respectively.","tags":null,"title":"Suhail Kamal","type":"authors"},{"authors":["admin"],"categories":null,"content":"  Data scientist often spent about 80% of data analysis process on cleaning and preparing data1. Worst still, cleaning and preparing the data is an iterative process. Hadley Wickham refer to this process of cleaning and preparing data as data tidying: structuring datasets to facilitate analysis. Therefore, it is very important to get the right tool to efficiently and quickly tidy any messy data and spend more time working on your analysis.\n  I mostly use Python for machine learning. But R is an exceptional tool for data manipulation, data visualization, and data analysis due to the many available packages in R developed mainly for data analysis. R is becoming the “de facto best tool” for data analysis. Therefore, we will use R to explore different ways to tidy data. In this part 1 series, we will gently start by exploring the Janitor2 package and see how it makes tidying data easy. In subsequent series, we will explore dplyr3 and tidyr4 packages which are the most complete packages for data manipulaton and tidying data.\n  Janitor Package The janitor package has user-friendly functions for tidying messy data. It provides functions for formating column names, detecting duplicate records, provide quick tabulations, and many more.\nFor the purpose of this series, we are going to use a pinguin dataset that has been deliberately messed.\nLoading the required packages\nlibrary(janitor) # CRAN v2.1.0 library(tidyverse) # CRAN v1.3.0 Loading the dataset from Github.\nmessy_penguins \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/BrunoGrandePhD/2020-11-14-rladies-workshop/rladies-tunis/learnr-tutorial/messy_penguins.csv\u0026quot;) Observing the data\nglimpse(messy_penguins) ## Rows: 344 ## Columns: 18 ## $ studyName \u0026lt;chr\u0026gt; \u0026quot;PAL0708\u0026quot;, \u0026quot;PAL0708\u0026quot;, \u0026quot;PAL0708\u0026quot;, \u0026quot;PAL0708\u0026quot;, \u0026quot;PA… ## $ `Sample Number` \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, … ## $ Species \u0026lt;chr\u0026gt; \u0026quot;Adelie Penguin (Pygoscelis adeliae)\u0026quot;, NA, NA, … ## $ Region \u0026lt;chr\u0026gt; \u0026quot;Anvers\u0026quot;, \u0026quot;Anvers\u0026quot;, \u0026quot;Anvers\u0026quot;, \u0026quot;Anvers\u0026quot;, \u0026quot;Anvers… ## $ Island \u0026lt;chr\u0026gt; NA, NA, \u0026quot;Torgersen\u0026quot;, NA, NA, NA, NA, NA, NA, NA… ## $ Stage \u0026lt;chr\u0026gt; \u0026quot;Adult, 2 Egg Stage\u0026quot;, \u0026quot;Adult, 1 Egg Stage\u0026quot;, \u0026quot;Ju… ## $ `Individual ID` \u0026lt;chr\u0026gt; \u0026quot;N1A1\u0026quot;, \u0026quot;N1A2\u0026quot;, \u0026quot;N2A1\u0026quot;, \u0026quot;N2A2\u0026quot;, \u0026quot;N3A1\u0026quot;, \u0026quot;N3A2\u0026quot;,… ## $ `Clutch Completion` \u0026lt;chr\u0026gt; \u0026quot;Yes\u0026quot;, \u0026quot;Yes\u0026quot;, \u0026quot;Yes\u0026quot;, \u0026quot;Yes\u0026quot;, \u0026quot;Yes\u0026quot;, \u0026quot;Yes\u0026quot;, \u0026quot;No\u0026quot;,… ## $ `Date Egg` \u0026lt;chr\u0026gt; \u0026quot;11/11/07\u0026quot;, \u0026quot;11/11/07\u0026quot;, \u0026quot;11/16/07\u0026quot;, \u0026quot;11/16/07\u0026quot;,… ## $ `Culmen Length (mm)` \u0026lt;chr\u0026gt; \u0026quot;39.1\u0026quot;, \u0026quot;39.5\u0026quot;, \u0026quot;40.3\u0026quot;, NA, \u0026quot;36.7\u0026quot;, \u0026quot;39.3\u0026quot;, \u0026quot;38… ## $ Nickname \u0026lt;chr\u0026gt; \u0026quot;Kamile\u0026quot;, \u0026quot;Dixie\u0026quot;, \u0026quot;Arian\u0026quot;, \u0026quot;Alexander\u0026quot;, \u0026quot;Dewi\u0026quot;… ## $ `Culmen Depth (mm)` \u0026lt;chr\u0026gt; \u0026quot;18.7\u0026quot;, \u0026quot;17.4\u0026quot;, \u0026quot;18\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;19.3\u0026quot;, \u0026quot;20.6\u0026quot;, \u0026quot;17.… ## $ `Flipper Length (mm)` \u0026lt;chr\u0026gt; \u0026quot;181\u0026quot;, \u0026quot;186\u0026quot;, \u0026quot;195\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;193\u0026quot;, \u0026quot;190\u0026quot;, \u0026quot;181\u0026quot;,… ## $ `Body Mass` \u0026lt;chr\u0026gt; \u0026quot;3750 g\u0026quot;, \u0026quot;3800g\u0026quot;, \u0026quot;3250\u0026quot;, NA, \u0026quot;3450 grams\u0026quot;, \u0026quot;3… ## $ Sex \u0026lt;chr\u0026gt; \u0026quot;M\u0026quot;, \u0026quot;FEMALE\u0026quot;, \u0026quot;FEMALE\u0026quot;, NA, \u0026quot;FEMALE\u0026quot;, \u0026quot;MALE\u0026quot;, … ## $ `Delta 15 N (o/oo)` \u0026lt;chr\u0026gt; NA, \u0026quot;8.94956\u0026quot;, \u0026quot;8.36821\u0026quot;, NA, \u0026quot;8.76651\u0026quot;, \u0026quot;8.664… ## $ `Delta 13 C (o/oo)` \u0026lt;chr\u0026gt; NA, \u0026quot;-24.69454\u0026quot;, \u0026quot;-25.33302\u0026quot;, NA, \u0026quot;-25.32426\u0026quot;, … ## $ Comments \u0026lt;chr\u0026gt; \u0026quot;Not enough blood for isotopes.\u0026quot;, NA, NA, \u0026quot;Adul… clean_names() function The clean_names() handles problematic variable names, returning only lowercase letters with underscore as separator, appends numbers to duplicated names, andles special characters and spaces, and converts “%” to “percent” to retain meaning. clean_names() can only be use on dataframe like objects, other objects such as named lists and vectors, make_clean_names() is used.\nLet us observe column names for our messy data\ncolnames(messy_penguins) ## [1] \u0026quot;studyName\u0026quot; \u0026quot;Sample Number\u0026quot; \u0026quot;Species\u0026quot; ## [4] \u0026quot;Region\u0026quot; \u0026quot;Island\u0026quot; \u0026quot;Stage\u0026quot; ## [7] \u0026quot;Individual ID\u0026quot; \u0026quot;Clutch Completion\u0026quot; \u0026quot;Date Egg\u0026quot; ## [10] \u0026quot;Culmen Length (mm)\u0026quot; \u0026quot;Nickname\u0026quot; \u0026quot;Culmen Depth (mm)\u0026quot; ## [13] \u0026quot;Flipper Length (mm)\u0026quot; \u0026quot;Body Mass\u0026quot; \u0026quot;Sex\u0026quot; ## [16] \u0026quot;Delta 15 N (o/oo)\u0026quot; \u0026quot;Delta 13 C (o/oo)\u0026quot; \u0026quot;Comments\u0026quot; We can see that, the column names structure is not uniform ( e.g “studyName”, “Sample Number”, “Delta 15 N (o/oo)”). So, to tidy these column names, we can use the clean_names() function to change them to uniform structure.\ntidy_penguins \u0026lt;- clean_names(messy_penguins) colnames(tidy_penguins) ## [1] \u0026quot;study_name\u0026quot; \u0026quot;sample_number\u0026quot; \u0026quot;species\u0026quot; ## [4] \u0026quot;region\u0026quot; \u0026quot;island\u0026quot; \u0026quot;stage\u0026quot; ## [7] \u0026quot;individual_id\u0026quot; \u0026quot;clutch_completion\u0026quot; \u0026quot;date_egg\u0026quot; ## [10] \u0026quot;culmen_length_mm\u0026quot; \u0026quot;nickname\u0026quot; \u0026quot;culmen_depth_mm\u0026quot; ## [13] \u0026quot;flipper_length_mm\u0026quot; \u0026quot;body_mass\u0026quot; \u0026quot;sex\u0026quot; ## [16] \u0026quot;delta_15_n_o_oo\u0026quot; \u0026quot;delta_13_c_o_oo\u0026quot; \u0026quot;comments\u0026quot; Now, all the column names are change to lower-case and have consistent structure. By default, clean_names() return snake-case like names, but you can specify other options such as the following cases:\n snake_case: “snake” lowerCamel: “lower_camel” or “small_camel” UpperCamel: “upper_camel” or “big_camel” ALL_CAPS: “all_caps” or “screaming_snake” lowerUPPER: “lower_upper” UPPERlower: “upper_lower” Sentence case: “sentence” Title Case: “title”  You can get details about any case here\nmessy_penguins %\u0026gt;% clean_names(case =\u0026quot;lower_camel\u0026quot;) %\u0026gt;% colnames() ## [1] \u0026quot;studyName\u0026quot; \u0026quot;sampleNumber\u0026quot; \u0026quot;species\u0026quot; \u0026quot;region\u0026quot; ## [5] \u0026quot;island\u0026quot; \u0026quot;stage\u0026quot; \u0026quot;individualId\u0026quot; \u0026quot;clutchCompletion\u0026quot; ## [9] \u0026quot;dateEgg\u0026quot; \u0026quot;culmenLengthMm\u0026quot; \u0026quot;nickname\u0026quot; \u0026quot;culmenDepthMm\u0026quot; ## [13] \u0026quot;flipperLengthMm\u0026quot; \u0026quot;bodyMass\u0026quot; \u0026quot;sex\u0026quot; \u0026quot;delta15NOOo\u0026quot; ## [17] \u0026quot;delta13COOo\u0026quot; \u0026quot;comments\u0026quot; To clean names, but leave some abbreviations to appear the way you want.\nmessy_penguins %\u0026gt;% clean_names(case = \u0026quot;lower_camel\u0026quot;, abbreviations = c(\u0026quot;ID\u0026quot;, \u0026quot;N\u0026quot;, \u0026quot;mm\u0026quot;)) %\u0026gt;% colnames() ## [1] \u0026quot;studyNAme\u0026quot; \u0026quot;sampleNUmber\u0026quot; \u0026quot;species\u0026quot; \u0026quot;region\u0026quot; ## [5] \u0026quot;island\u0026quot; \u0026quot;stage\u0026quot; \u0026quot;individualID\u0026quot; \u0026quot;clutchCompletion\u0026quot; ## [9] \u0026quot;dateEgg\u0026quot; \u0026quot;culmenLengthMm\u0026quot; \u0026quot;nIckname\u0026quot; \u0026quot;culmenDepthMm\u0026quot; ## [13] \u0026quot;flipperLengthMm\u0026quot; \u0026quot;bodyMass\u0026quot; \u0026quot;sex\u0026quot; \u0026quot;delta15NOOo\u0026quot; ## [17] \u0026quot;delta13COOo\u0026quot; \u0026quot;comments\u0026quot; You can also restore column names to Title Case, e.g., for plotting\ntidy_penguins %\u0026gt;% clean_names( case = \u0026quot;title\u0026quot;) %\u0026gt;% colnames() ## [1] \u0026quot;Study Name\u0026quot; \u0026quot;Sample Number\u0026quot; \u0026quot;Species\u0026quot; ## [4] \u0026quot;Region\u0026quot; \u0026quot;Island\u0026quot; \u0026quot;Stage\u0026quot; ## [7] \u0026quot;Individual Id\u0026quot; \u0026quot;Clutch Completion\u0026quot; \u0026quot;Date Egg\u0026quot; ## [10] \u0026quot;Culmen Length Mm\u0026quot; \u0026quot;Nickname\u0026quot; \u0026quot;Culmen Depth Mm\u0026quot; ## [13] \u0026quot;Flipper Length Mm\u0026quot; \u0026quot;Body Mass\u0026quot; \u0026quot;Sex\u0026quot; ## [16] \u0026quot;Delta 15 n o Oo\u0026quot; \u0026quot;Delta 13 c o Oo\u0026quot; \u0026quot;Comments\u0026quot; For vectors, we can use make_clean_names functions.\nx \u0026lt;- structure(1:4, names = c(\u0026quot;This is first\u0026quot;, \u0026quot;this issecond\u0026quot;, \u0026quot;3rd\u0026quot;, \u0026quot;FinalChoice\u0026quot;)) x ## This is first this issecond 3rd FinalChoice ## 1 2 3 4 names(x) \u0026lt;- make_clean_names(names(x)) # `x` is added to names that start with number x ## this_is_first this_issecond x3rd final_choice ## 1 2 3 4  tabyl() function This function is an alternative to the table() function from base-R. The function generate a frequency table from either a dataframe or vector.\nstudynames \u0026lt;- messy_penguins %\u0026gt;% clean_names() %\u0026gt;% tabyl(study_name) # %\u0026gt;% #adorn_pct_formatting(digits = 0, affix_sign = TRUE) # creates a percentage column studynames ## study_name n percent ## PAL0708 110 0.3197674 ## PAL0809 114 0.3313953 ## PAL0910 120 0.3488372 After tabyl() function, we can use the janitor’s family of adorn_functions to format the resultant dataframes.\nstudynames \u0026lt;- messy_penguins %\u0026gt;% clean_names() %\u0026gt;% tabyl(study_name) %\u0026gt;% adorn_pct_formatting(digits = 0, affix_sign = TRUE) %\u0026gt;% # format the percentage column adorn_totals(where = \u0026quot;row\u0026quot;) #%\u0026gt;% #Add totals studynames ## study_name n percent ## PAL0708 110 32% ## PAL0809 114 33% ## PAL0910 120 35% ## Total 344 - Other adorn functions are :\n adorn_pct_formatting Format a data.frame of decimals as percentages adorn_rounding Round the numeric columns in a data.frame adorn_totals Add a totals row and/or column to a data.frame. adorn_ns Add underlying Ns to a tabyl displaying percentages  mtcars %\u0026gt;% tabyl(am, cyl) %\u0026gt;% adorn_percentages(\u0026quot;col\u0026quot;) %\u0026gt;% adorn_totals(where = c(\u0026quot;row\u0026quot;,\u0026quot;col\u0026quot;)) %\u0026gt;% adorn_pct_formatting(digits = 0) %\u0026gt;% adorn_rounding(digits = 3) %\u0026gt;% adorn_ns(position = \u0026quot;front\u0026quot;) # %\u0026gt;%  ## am 4 6 8 Total ## 0 3 (27%) 4 (57%) 12 (86%) 19 (170%) ## 1 8 (73%) 3 (43%) 2 (14%) 13 (130%) ## Total 11 (100%) 7 (100%) 14 (100%) 32 (300%) Let use compare with Base-R table() function:\ntidy_names \u0026lt;- messy_penguins %\u0026gt;% clean_names() table(tidy_names$study_name) ## ## PAL0708 PAL0809 PAL0910 ## 110 114 120 If you have a vector, the function also works the same\nAge \u0026lt;- c(23, 24, 24, 25, 23,26, 25) tabyl(Age) ## Age n percent ## 23 2 0.2857143 ## 24 2 0.2857143 ## 25 2 0.2857143 ## 26 1 0.1428571  remove_empty() This functions usage is straight foward. It simply removes any colum/rows from a data.frame or matrix that contain all “NA” as their entries.\nempty \u0026lt;- messy_penguins %\u0026gt;% remove_empty() # this will remove both empty rows and columns by default.  ## value for \u0026quot;which\u0026quot; not specified, defaulting to c(\u0026quot;rows\u0026quot;, \u0026quot;cols\u0026quot;) empty \u0026lt;- messy_penguins %\u0026gt;% remove_empty(which = \u0026quot;rows\u0026quot;, quiet = TRUE) # specify row or column and use quite argument to supress messages be suppressed (TRUE) or printed (FALSE) indicating the summary of empty columns or rows removed?  remove-constant() Sometimes, we may need to find columns that have the same values, we can use remove-constant() to to do that.\ndata.frame(A=1, B=1:3, c= c(3,3,3))  ## A B c ## 1 1 1 3 ## 2 1 2 3 ## 3 1 3 3 data.frame(A=1, B=1:3, c= c(3,3,3)) %\u0026gt;% dplyr::select_at(setdiff(names(.), names(remove_constant(.)))) %\u0026gt;%unique() ## A c ## 1 1 3  get_dupes() to Remove duplicates Sometimes our data may contains duplicates that we may not like. So, we need to find and possibly remove them if any. get_dupes() returns a records (and inserts a count of duplicates) so that we can deal with identfied cases accordingly.\nmessy_penguins %\u0026gt;% clean_names() %\u0026gt;% get_dupes() # This dataset no any duplicate. ## No variable names specified - using all columns. ## No duplicate combinations found of: study_name, sample_number, species, region, island, stage, individual_id, clutch_completion, date_egg, ... and 9 other variables ## # A tibble: 0 x 19 ## # … with 19 variables: study_name \u0026lt;chr\u0026gt;, sample_number \u0026lt;dbl\u0026gt;, species \u0026lt;chr\u0026gt;, ## # region \u0026lt;chr\u0026gt;, island \u0026lt;chr\u0026gt;, stage \u0026lt;chr\u0026gt;, individual_id \u0026lt;chr\u0026gt;, ## # clutch_completion \u0026lt;chr\u0026gt;, date_egg \u0026lt;chr\u0026gt;, culmen_length_mm \u0026lt;chr\u0026gt;, ## # nickname \u0026lt;chr\u0026gt;, culmen_depth_mm \u0026lt;chr\u0026gt;, flipper_length_mm \u0026lt;chr\u0026gt;, ## # body_mass \u0026lt;chr\u0026gt;, sex \u0026lt;chr\u0026gt;, delta_15_n_o_oo \u0026lt;chr\u0026gt;, delta_13_c_o_oo \u0026lt;chr\u0026gt;, ## # comments \u0026lt;chr\u0026gt;, dupe_count \u0026lt;int\u0026gt; Tip: call clean_names() every time you read in a new data set to automatically clean column names.\n body { text-align: justify}     Tidy Data : Hadley Wikham↩︎\n The janitor package↩︎\n dplyr the grammar of data manipulation↩︎\n Tidyr↩︎\n   ","date":1610150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610150400,"objectID":"b1cb75b39a1a11287c58fa537ad17a3e","permalink":"https://hausanlp.github.io/post/tidying-messy-data/","publishdate":"2021-01-09T00:00:00Z","relpermalink":"/post/tidying-messy-data/","section":"post","summary":"Data scientist often spent about 80% of data analysis process on cleaning and preparing data1. Worst still, cleaning and preparing the data is an iterative process. Hadley Wickham refer to this process of cleaning and preparing data as data tidying: structuring datasets to facilitate analysis.","tags":["Rstats","Data Science"],"title":"How to Tidy Messy Data  Part 1)","type":"post"},{"authors":null,"categories":null,"content":"  ","date":1609059600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609059600,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://hausanlp.github.io/event/example/","publishdate":"2020-12-22T00:00:00Z","relpermalink":"/event/example/","section":"event","summary":"In this weekly session, we meet and discuss our progress and any interesting idea","tags":null,"title":"Weekly Meeting","type":"talk"},{"authors":["Idris Abdulmumin","Bashir Shehu Galadanci","Abubakar Isa"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"fdb012eb45d710028b0607e87e7ffc2a","permalink":"https://hausanlp.github.io/publication/abdulmumin-2020-aa/","publishdate":"2020-11-07T05:03:22.908141Z","relpermalink":"/publication/abdulmumin-2020-aa/","section":"publication","summary":"Improving neural machine translation (NMT) models using the back-translations of the monolingual target data (synthetic parallel data) is currently the state-of-the-art approach for training improved translation systems. The quality of the backward system - which is trained on the available parallel data and used for the back-translation - has been shown in many studies to affect the performance of the final NMT model. In low resource conditions, the available parallel data is usually not enough to train a backward model that can produce the qualitative synthetic data needed to train a standard translation model. This work proposes a self-training strategy where the output of the backward model is used to improve the model itself through the forward translation technique. The technique was shown to improve baseline low resource IWSLT'14 English-German and IWSLT'15 English-Vietnamese backward translation models by 11.06 and 1.5 BLEUs respectively. The synthetic data generated by the improved English-German backward model was used to train a forward model which out-performed another forward model trained using standard back-translation by 2.7 BLEU.","tags":null,"title":"Using Self-Training to Improve Back-Translation in Low Resource Neural Machine Translation","type":"publication"},{"authors":["Wilhelmina Nekoto","Vukosi Marivate","Tshinondiwa Matsila","Timi Fasubaa","Tajudeen Kolawole","Taiwo Fagbohungbe","Solomon Oluwole Akinola","Shamsuddee Hassan Muhammad","Salomon Kabongo","Salomey Osei"," others"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"fc22c21a252c0b9895808777f9450f4f","permalink":"https://hausanlp.github.io/publication/nekoto-2020-participatory/","publishdate":"2020-11-06T15:53:24.936538Z","relpermalink":"/publication/nekoto-2020-participatory/","section":"publication","summary":"","tags":null,"title":"Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages","type":"publication"},{"authors":["Idris Abdulmumin","Bashir Shehu Galadanci","Aliyu Garba"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"8733e086181392f23633b4c2f8606251","permalink":"https://hausanlp.github.io/publication/abdulmumin-2019-ab/","publishdate":"2020-11-07T05:03:22.911082Z","relpermalink":"/publication/abdulmumin-2019-ab/","section":"publication","summary":"An effective method to generate a large number of parallel sentences for training improved neural machine translation (NMT) systems is the use of back-translations of the target-side monolingual data. The method was not able to utilize the available huge amount of monolingual data because of the inability of models to differentiate between the authentic and synthetic parallel data. Tagging, or using gates, has been used to enable translation models to distinguish between synthetic and authentic data, improving standard back-translation and also enabling the use of iterative back-translation on language pairs that under-performed using standard back-translation. This work presents pre-training and fine-tuning as a simplified but more effective approach of differentiating between the two data. The approach - tag-less back-translation - trains the model on the synthetic data and fine-tunes it on the authentic data. Experiments have shown the approach to outperform the baseline and standard back-translation by 4.0 and 0.7 BLEU respectively on low resource English-Vietnamese NMT. While the need for tagging (noising) the dataset has been removed, the technique outperformed tagged back-translation by 0.4 BLEU. The approach reached the best scores in less training time than the standard and tagged back-translation approaches.","tags":null,"title":"Tag-less Back-Translation","type":"publication"},{"authors":["Idris Abdulmumin","Bashir Shehu Galadanci"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"f12c75e495862494292eed37cbb5f7b5","permalink":"https://hausanlp.github.io/publication/abdulmumin-2019-aa/","publishdate":"2020-11-07T05:03:22.913105Z","relpermalink":"/publication/abdulmumin-2019-aa/","section":"publication","summary":"Words embedding (distributed word vector representations) have become an essential component of many natural language processing (NLP) tasks such as machine translation, sentiment analysis, word analogy, named entity recognition and word similarity. Despite this, the only work that provides word vectors for Hausa language is that of Bojanowski et al. [1] trained using fastText, consisting of only a few words vectors. This work presents words embedding models using Word2Vec's Continuous Bag of Words (CBoW) and Skip Gram (SG) models. The models, hauWE (Hausa Words Embedding), are bigger and better than the only previous model, making them more useful in NLP tasks. To compare the models, they were used to predict the 10 most similar words to 30 randomly selected Hausa words. hauWE CBoW's 88.7% and hauWE SG's 79.3% prediction accuracy greatly outperformed Bojanowski et al. [1]'s 22.3%.","tags":null,"title":"hauWE: Hausa Words Embedding for Natural Language Processing","type":"publication"},{"authors":["admin"],"categories":null,"content":"Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"cf4b2602f9ee846aab64791952734587","permalink":"https://hausanlp.github.io/publications/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publications/preprint/","section":"publications","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publications"},{"authors":null,"categories":null,"content":"This\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e593cfe6437b3d2b4638b85ca9596968","permalink":"https://hausanlp.github.io/project/hausacorpus/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/hausacorpus/","section":"project","summary":"This is a collection of Hausa corpus","tags":["corpus"],"title":"Hausa Corpus","type":"project"},{"authors":null,"categories":null,"content":"This\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e590569f5b639057d7784f6862421a2a","permalink":"https://hausanlp.github.io/project/hausatranslation/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/hausatranslation/","section":"project","summary":"This is text classification task","tags":["translation"],"title":"Hausa Machine Translation","type":"project"},{"authors":null,"categories":null,"content":"This\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"6b24ca70bf8764702eb94ac9bf49e8e3","permalink":"https://hausanlp.github.io/project/hausaspeech/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/hausaspeech/","section":"project","summary":"This is text classification task","tags":["speech"],"title":"Hausa Speech Recognition","type":"project"},{"authors":null,"categories":null,"content":"This\n\u0026mdash; date: \u0026ldquo;2020-02-27T00:00:00+01:00\u0026rdquo; external_link: \u0026quot;\u0026quot; image: caption: \u0026quot;\u0026quot; focal_point: Smart #links: #- icon: twitter # icon_pack: fab # name: Follow # url: https://twitter.com/georgecushen #slides: example-slides summary: This project aims to develop Hausa language resource for natural language processing task such as Hausa Social Media Corpus, Hausa Sentiment Lexicon , HausaNER , and POS. tags: - NLP title: Hausa Text Classification url_code: url_pdf: \u0026quot;\u0026quot; url_slides: \u0026quot;\u0026quot; url_video: \u0026quot;\u0026quot; \u0026mdash;  This project involves people that are interested in Hausa text classification task such as sentiment analysis.    body { text-align: justify}   ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"44ed312f386d3240f8cf93a10b01aa93","permalink":"https://hausanlp.github.io/project/sanalysis/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/sanalysis/","section":"project","summary":"This is text classification task","tags":["classification"],"title":"Sentiment Analysis","type":"project"},{"authors":["admin","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"cf04597e2a659887077fbca9d04925e4","permalink":"https://hausanlp.github.io/publications/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publications/journal-article/","section":"publications","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publications"},{"authors":["Jaafar Zubairu Maitama","Usman Haruna","Abdullahi Ya'u Gambo","Bimba Andrew Thomas","Norisma Binti Idris","Abdulsalam Ya'u Gital","Adamu I Abubakar"],"categories":null,"content":"The rapid increase in using non-standard words (NSWs) in communication through the social media is causing difficulties in understanding contents of the text messages. In addition, it affects the performance of several natural language processing (NLP) task such as machine translation, information retrievals, summarization and etc. In this study, we present an automatic text normalization system on Facebook chatting based on Hausa language. The proposed algorithm manually developed dictionary that employ normalization of each non-standard word with its equivalent standard word. This is accomplished through modification of the technique employed by [1] to fit Hausa NSWs' formation. It was found that our proposed algorithm was able to normalized Hausa NSWs with an accuracy of 100%The results of this research can facilitate comprehensive communication via Facebook using Hausa language.\n body { text-align: justify}  ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"3b383a401f060ae834425f898fe1ea68","permalink":"https://hausanlp.github.io/publication/maitama-2014-text/","publishdate":"2019-10-28T10:46:20.252434Z","relpermalink":"/publication/maitama-2014-text/","section":"publication","summary":"The rapid increase in using non-standard words (NSWs) in communication through the social media is causing difficulties in understanding contents of the text messages. In addition, it affects the performance of several natural language processing (NLP) task such as machine translation, information retrievals, summarization and etc.","tags":null,"title":"Text normalization algorithm for facebook chats in hausa language","type":"publication"},{"authors":["admin","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"f31f77da2d0a6fd63afe7e520495d729","permalink":"https://hausanlp.github.io/publications/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publications/conference-paper/","section":"publications","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publications"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://hausanlp.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://hausanlp.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]